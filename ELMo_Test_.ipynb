{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8vbMo7IIjqn",
    "outputId": "223b2760-86da-40c8-a2d4-5ee7498fc576"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gulja\\python 3.7.3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\gulja\\python 3.7.3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "c:\\users\\gulja\\python 3.7.3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from wordcloud) (1.18.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from wordcloud) (6.2.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from wordcloud) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\gulja\\python 3.7.3\\python.exe -m pip install --upgrade pip' command.\n",
      "c:\\users\\gulja\\python 3.7.3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gulja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "ERROR: Could not find a version that satisfies the requirement subprocess (from versions: none)\n",
      "ERROR: No matching distribution found for subprocess\n",
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\gulja\\python 3.7.3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install wordcloud\n",
    "#import standard visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split #split\n",
    "from sklearn.metrics import accuracy_score #metrics\n",
    "\n",
    "#tools for hyperparameters search\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing \n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, recall_score, precision_score, f1_score,roc_auc_score\n",
    "from sklearn.metrics import plot_roc_curve,roc_curve,auc \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# download the stopwords from NLTK\n",
    "import nltk                                # Python library for NLP\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import regexp_tokenize   # module for tokenizing strings\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "!pip install subprocess\n",
    "from subprocess import check_output \n",
    "# print(check_output([\"ls\", \"./\"]).decode(\"utf8\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gulja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gulja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordsegment in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\gulja\\python 3.7.3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "# ML Libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# NLTK\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# DL Libraries\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Activation,  Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import SGD\n",
    "!pip install wordsegment\n",
    "from wordsegment import load, segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rOSQ1fs1IrYV"
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Model \n",
    "from keras.layers import Dense, Embedding, Input \n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, Lambda \n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "import keras.layers as layers\n",
    "from keras.engine import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (3.0.7)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (from openpyxl) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\gulja\\python 3.7.3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install xlrd==1.2.0\n",
    "# !pip uninstall xlrd\n",
    "# pip install xlrd ==1.2.0\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordsegment in c:\\users\\gulja\\python 3.7.3\\lib\\site-packages (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\gulja\\python 3.7.3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Activation,  Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import SGD\n",
    "!pip install wordsegment\n",
    "from wordsegment import load, segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "sk3UdT3IKCJe",
    "outputId": "f43000bb-8348-4614-cc86-51aa33c79f0f"
   },
   "outputs": [],
   "source": [
    "hs_data = pd.read_excel('dataNLP.xlsx',engine = 'openpyxl')\n",
    "# hs_data = pd.read_excel(\n",
    "#      os.path.join(APP_PATH, \"Data\", \"aug_latest.xlsm\"),\n",
    "#      engine='openpyxl',\n",
    "# )\n",
    "hs_data.to_csv(r'data.csv', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hsid</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp;amp; as a man you should always take the trash out...</td>\n",
       "      <td>woman shouldnt complain clean house. amp man away take trash ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!</td>\n",
       "      <td>boy dat cold ... type don bad coffin dat hoe st place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit</td>\n",
       "      <td>dawn  ever fuck bitch sta cri confuse shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she look like a tranny</td>\n",
       "      <td>look like trans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &amp;#57361;</td>\n",
       "      <td>shit hear might true might fake bitch told ya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>25291</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @20_Pearls @corey_emanuel right! His TL is trash &amp;#8230;. Now, mine? Bible scriptures and hymns&amp;#8221;</td>\n",
       "      <td>you muthafin lie right tl trash mine bill scripture hymn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>25292</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>you've gone and broke the wrong heart baby, and drove me redneck crazy</td>\n",
       "      <td>you gone broke wrong hea baby drove redneck crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>25294</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I aint fuckin dis up again</td>\n",
       "      <td>young buck wan na eat .. dat nigguh like aint fucking di</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>25295</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "      <td>you got wild bitch telling lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>25296</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful color combination of pink, orange, yellow &amp;amp; white. A Coll http://t.co/H0dYEBvnZB</td>\n",
       "      <td>rural nta eleven alia beauty color combine pink orange yellow amp white. coll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hsid  count  hate_speech  offensive_language  neither  class  \\\n",
       "0      0      3      0            0                   3        1       \n",
       "1      1      3      0            3                   0        0       \n",
       "2      2      3      0            3                   0        0       \n",
       "3      3      3      0            2                   1        0       \n",
       "4      4      6      0            6                   0        0       \n",
       "...   ..     ..     ..           ..                  ..       ..       \n",
       "24778  25291  3      0            2                   1        0       \n",
       "24779  25292  3      0            1                   2        1       \n",
       "24780  25294  3      0            3                   0        0       \n",
       "24781  25295  6      0            6                   0        0       \n",
       "24782  25296  3      0            0                   3        1       \n",
       "\n",
       "                                                                                                                                                    tweet  \\\n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...         \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!                                                                \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit                             \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she look like a tranny                                                                                       \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;            \n",
       "...                                                                                                                                          ...            \n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @20_Pearls @corey_emanuel right! His TL is trash &#8230;. Now, mine? Bible scriptures and hymns&#8221;   \n",
       "24779  you've gone and broke the wrong heart baby, and drove me redneck crazy                                                                               \n",
       "24780  young buck wanna eat!!.. dat nigguh like I aint fuckin dis up again                                                                                  \n",
       "24781  youu got wild bitches tellin you lies                                                                                                                \n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful color combination of pink, orange, yellow &amp; white. A Coll http://t.co/H0dYEBvnZB                      \n",
       "\n",
       "                                                                        tweet_updated  \n",
       "0        woman shouldnt complain clean house. amp man away take trash ...              \n",
       "1        boy dat cold ... type don bad coffin dat hoe st place                         \n",
       "2        dawn  ever fuck bitch sta cri confuse shit                                    \n",
       "3        look like trans                                                               \n",
       "4        shit hear might true might fake bitch told ya                                 \n",
       "...                                                ...                                 \n",
       "24778   you muthafin lie right tl trash mine bill scripture hymn                       \n",
       "24779   you gone broke wrong hea baby drove redneck crazy                              \n",
       "24780   young buck wan na eat .. dat nigguh like aint fucking di                       \n",
       "24781   you got wild bitch telling lie                                                 \n",
       "24782   rural nta eleven alia beauty color combine pink orange yellow amp white. coll  \n",
       "\n",
       "[24783 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_tweet_text(tweet, user=False):  # Preprocessing steps to remove punctuations, #, @ , LINKS from the text\n",
    "    apostrophes = {\n",
    "        '\\xe2\\x80\\x99 s': 'is',\n",
    "        \"hasn't\": ' has not',\n",
    "        \"didn't\": ' did not',\n",
    "        \"haven't\": ' have not',\n",
    "        \"'s\": ' is',\n",
    "        \"'re\": ' are',\n",
    "        \"'ll\": ' will',\n",
    "        \"i'm\": ' i am',\n",
    "        \"i've\": ' i have',\n",
    "        \"'d\": ' would',\n",
    "        \"i'm\": 'I am',\n",
    "        \"I'm\": 'I am',\n",
    "        \"won't\": 'will not',\n",
    "        \"'ve\": ' have',\n",
    "        \"can't\": 'cannot',\n",
    "        \"couldn't\": 'could not',\n",
    "        }\n",
    "    tweet = str(tweet).lower()\n",
    "    for (k, v) in apostrophes.items():\n",
    "        tweet = tweet.replace(k, v)\n",
    "\n",
    "    if user == True:\n",
    "        tweet = re.sub(r'\\@\\w+', 'USER', tweet)  # replace user name with USER\n",
    "        tweet = re.sub('[^A-Za-z0-9]+', ' ', tweet)\n",
    "        tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", 'LINK', tweet,\n",
    "                       flags=re.MULTILINE)  # replace link with LINK\n",
    "                                            # Remove urls\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(r'\\#', '', tweet)  # Remove '#' from tweet\n",
    "        tweet = tweet.replace('  ', ' ')\n",
    "        tweet = tweet.replace('\\xe2\\x80\\xa2', ' ')\n",
    "\n",
    "    tweet = tweet.translate(str.maketrans('', '',\n",
    "                            string.punctuation))  # Remove punctuations\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_vlRphUUJ0Kj"
   },
   "outputs": [],
   "source": [
    "df['tweet_updated']   = df['tweet_updated'] .apply(preprocessing_tweet_text, False)\n",
    "\n",
    "maxlen = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          woman shouldnt complain clean house amp man away take trash                \n",
       "1          boy dat cold  type don bad coffin dat hoe st place                         \n",
       "2          dawn  ever fuck bitch sta cri confuse shit                                 \n",
       "3          look like trans                                                            \n",
       "4          shit hear might true might fake bitch told ya                              \n",
       "                              ...                                                     \n",
       "24778     you muthafin lie right tl trash mine bill scripture hymn                    \n",
       "24779     you gone broke wrong hea baby drove redneck crazy                           \n",
       "24780     young buck wan na eat  dat nigguh like aint fucking di                      \n",
       "24781     you got wild bitch telling lie                                              \n",
       "24782     rural nta eleven alia beauty color combine pink orange yellow amp white coll\n",
       "Name: tweet_updated, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_updated'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aBZfZrGrpenI"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(df['tweet_updated'].values)\n",
    "X = tokenizer.texts_to_sequences(df['tweet_updated'].values)\n",
    "max_fatures = len(tokenizer.word_index) + 1\n",
    "X = pad_sequences(maxlen=maxlen, sequences=X, padding=\"post\", value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  285,   892,   621, ...,     0,     0,     0],\n",
       "       [  102,    86,   483, ...,     0,     0,     0],\n",
       "       [  519,   108,     5, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  303,  1633,    75, ...,     0,     0,     0],\n",
       "       [  125,    10,   882, ...,     0,     0,     0],\n",
       "       [ 4720, 12576, 12577, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "24778    0\n",
       "24779    1\n",
       "24780    0\n",
       "24781    0\n",
       "24782    1\n",
       "Name: class, Length: 24783, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(df['class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open(\"glove.6B.100d.txt\" , encoding=\"utf8\" )\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((max_fatures, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "# kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon()) #we use epsilon, to handle 'divided by zero' condition.\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          1257900   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 100, 128)          84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 1,383,726\n",
      "Trainable params: 1,383,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model preparation\n",
    "\n",
    "embedding_dim=100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures,\n",
    "                    embedding_dim,#100\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=X.shape[1]))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy',f1_m,precision_m, recall_m])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13283 samples, validate on 3321 samples\n",
      "Epoch 1/5\n",
      "13283/13283 [==============================] - 339s 26ms/step - loss: 0.1750 - acc: 0.9266 - f1_m: 0.9266 - precision_m: 0.9266 - recall_m: 0.9266 - val_loss: 0.1146 - val_acc: 0.9557 - val_f1_m: 0.9557 - val_precision_m: 0.9557 - val_recall_m: 0.9557\n",
      "Epoch 2/5\n",
      "13283/13283 [==============================] - 339s 26ms/step - loss: 0.0965 - acc: 0.9648 - f1_m: 0.9648 - precision_m: 0.9648 - recall_m: 0.9648 - val_loss: 0.1089 - val_acc: 0.9584 - val_f1_m: 0.9584 - val_precision_m: 0.9584 - val_recall_m: 0.9584\n",
      "Epoch 3/5\n",
      "13283/13283 [==============================] - 339s 25ms/step - loss: 0.0548 - acc: 0.9812 - f1_m: 0.9812 - precision_m: 0.9812 - recall_m: 0.9812 - val_loss: 0.1721 - val_acc: 0.9395 - val_f1_m: 0.9395 - val_precision_m: 0.9395 - val_recall_m: 0.9395\n",
      "Epoch 4/5\n",
      "13283/13283 [==============================] - 342s 26ms/step - loss: 0.0348 - acc: 0.9886 - f1_m: 0.9886 - precision_m: 0.9886 - recall_m: 0.9886 - val_loss: 0.1868 - val_acc: 0.9482 - val_f1_m: 0.9482 - val_precision_m: 0.9482 - val_recall_m: 0.9482\n",
      "Epoch 5/5\n",
      "13283/13283 [==============================] - 281s 21ms/step - loss: 0.0209 - acc: 0.9937 - f1_m: 0.9937 - precision_m: 0.9937 - recall_m: 0.9937 - val_loss: 0.2331 - val_acc: 0.9461 - val_f1_m: 0.9461 - val_precision_m: 0.9461 - val_recall_m: 0.9461\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "history=model.fit(X_train, Y_train, epochs = 5,  validation_split = 0.2, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8179/8179 [==============================] - 15s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "y_pred= model.predict(X_test, verbose=1)\n",
    "y_pred_array = np.zeros(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find class with highest probability\n",
    "\n",
    "for i in range(0, y_pred.shape[0]):\n",
    "    label_predict = np.argmax(y_pred[i]) # column with max probability\n",
    "    y_pred_array[i] = label_predict\n",
    "\n",
    "# convert to integers\n",
    "\n",
    "y_pred_array = y_pred_array.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_array = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Find class with 1\n",
    "\n",
    "for i in range(0, Y_test.shape[0]):\n",
    "    label_predict = np.argmax(Y_test[i])\n",
    "    y_test_array[i] = label_predict\n",
    "\n",
    "y_test_array = y_test_array.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      6800\n",
      "           1       0.83      0.77      0.80      1379\n",
      "\n",
      "    accuracy                           0.94      8179\n",
      "   macro avg       0.89      0.87      0.88      8179\n",
      "weighted avg       0.93      0.94      0.93      8179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_array, y_pred_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ELMo Test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
